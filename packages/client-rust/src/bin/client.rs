use reqwest::Client;
use serde::{Deserialize, Serialize};
use serde_json::{json, Value};
use std::collections::HashMap;
use std::fs;
use std::time::Duration;
use tokio::time::sleep;
use uuid::Uuid;

use open_xiaoai::services::connect::data::{Event, Response};

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Config {
    mode: String, // "direct" or "proxy"
    openai: OpenAIConfig,
    #[serde(rename = "serverProxy")]
    server_proxy: Option<ServerProxyConfig>,
    prompt: PromptConfig,
    audio: Option<AudioConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct OpenAIConfig {
    #[serde(rename = "baseURL")]
    base_url: String,
    #[serde(rename = "apiKey")]
    api_key: String,
    model: String,
    timeout: Option<u64>,
    #[serde(rename = "maxTokens")]
    max_tokens: Option<u32>,
    temperature: Option<f32>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct ServerProxyConfig {
    #[serde(rename = "baseURL")]
    base_url: String,
    timeout: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct PromptConfig {
    system: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct AudioConfig {
    #[serde(rename = "sampleRate")]
    sample_rate: u32,
    channels: u32,
    format: String,
}

pub enum LLMService {
    Direct(DirectLLMService),
    Server(ServerProxyService),
}

impl LLMService {
    async fn call_llm(&self, instruction: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
        match self {
            LLMService::Direct(service) => service.call_llm(instruction).await,
            LLMService::Server(service) => service.call_llm(instruction).await,
        }
    }
}

pub struct DirectLLMService {
    config: OpenAIConfig,
    client: Client,
    system_prompt: String,
}

impl DirectLLMService {
    pub fn new(config: OpenAIConfig, system_prompt: String) -> Self {
        let timeout = Duration::from_secs(config.timeout.unwrap_or(30));
        let client = Client::builder()
            .timeout(timeout)
            .build()
            .expect("Failed to create HTTP client");

        Self {
            config,
            client,
            system_prompt,
        }
    }

    async fn call_llm(&self, instruction: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
        let url = format!("{}/chat/completions", self.config.base_url);
        
        let messages = vec![
            json!({
                "role": "system",
                "content": self.system_prompt
            }),
            json!({
                "role": "user", 
                "content": instruction
            })
        ];

        let body = json!({
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature.unwrap_or(0.7),
            "max_tokens": self.config.max_tokens.unwrap_or(1000)
        });

        println!("ðŸ¤– [DIRECT] Calling LLM: {}", instruction);
        
        let response = self.client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .json(&body)
            .send()
            .await?;

        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(format!("LLM API error: {}", error_text).into());
        }

        let response_json: Value = response.json().await?;
        
        if let Some(choices) = response_json.get("choices").and_then(|c| c.as_array()) {
            if let Some(first_choice) = choices.first() {
                if let Some(message) = first_choice.get("message") {
                    if let Some(content) = message.get("content").and_then(|c| c.as_str()) {
                        println!("âœ… [DIRECT] LLM response: {}", content);
                        return Ok(content.to_string());
                    }
                }
            }
        }

        Err("Invalid LLM response format".into())
    }
}

pub struct ServerProxyService {
    config: ServerProxyConfig,
    client: Client,
    client_id: String,
    headers: HashMap<String, String>,
}

impl ServerProxyService {
    pub fn new(config: ServerProxyConfig) -> Self {
        let timeout = Duration::from_secs(config.timeout.unwrap_or(30));
        let client = Client::builder()
            .timeout(timeout)
            .build()
            .expect("Failed to create HTTP client");

        let mut headers = HashMap::new();
        
        // Add Cloudflare Access Service Token headers if available
        if let Ok(client_id) = std::env::var("CF_ACCESS_CLIENT_ID") {
            headers.insert("CF-Access-Client-Id".to_string(), client_id);
        }
        if let Ok(client_secret) = std::env::var("CF_ACCESS_CLIENT_SECRET") {
            headers.insert("CF-Access-Client-Secret".to_string(), client_secret);
        }

        Self {
            config,
            client,
            client_id: Uuid::new_v4().to_string(),
            headers,
        }
    }

    async fn send_request(&self, method: &str, path: &str, body: Option<Value>) -> Result<Value, Box<dyn std::error::Error + Send + Sync>> {
        let url = format!("{}{}", self.config.base_url, path);
        
        let mut request = match method {
            "GET" => self.client.get(&url),
            "POST" => self.client.post(&url),
            _ => return Err("Unsupported HTTP method".into()),
        };

        // Add custom headers
        for (key, value) in &self.headers {
            request = request.header(key, value);
        }

        if let Some(body) = body {
            request = request.header("Content-Type", "application/json").json(&body);
        }

        let response = request.send().await?;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(format!("HTTP {} error: {}", status, error_text).into());
        }

        let result: Value = response.json().await?;
        Ok(result)
    }

    pub async fn register(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let body = json!({
            "clientId": self.client_id
        });

        let _response = self.send_request("POST", "/register", Some(body)).await?;
        println!("âœ… [PROXY] Client registered: {}", self.client_id);
        Ok(())
    }

    pub async fn send_event(&self, event: &Event) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        let body = json!(event);
        let _response = self.send_request("POST", "/events", Some(body)).await?;
        Ok(())
    }

    pub async fn poll_commands(&self) -> Result<Vec<Response>, Box<dyn std::error::Error + Send + Sync>> {
        let path = format!("/commands/{}", self.client_id);
        let response = self.send_request("GET", &path, None).await?;
        
        if let Some(commands) = response.get("commands") {
            let commands: Vec<Response> = serde_json::from_value(commands.clone())?;
            if !commands.is_empty() {
                println!("ðŸ“¨ [PROXY] Received {} commands", commands.len());
            }
            Ok(commands)
        } else {
            Ok(vec![])
        }
    }

    async fn call_llm(&self, instruction: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
        println!("ðŸŒ [PROXY] Sending instruction to server: {}", instruction);
        
        // Send instruction event to server
        let event = Event::new("instruction", json!({
            "text": instruction,
            "clientId": self.client_id
        }));
        
        self.send_event(&event).await?;
        println!("âœ… [PROXY] Instruction sent, waiting for response...");
        
        // Poll for response (try for up to 30 seconds)
        for attempt in 1..=30 {
            sleep(Duration::from_secs(1)).await;
            
            match self.poll_commands().await {
                Ok(commands) => {
                    for command in commands {
                        if let Some(action) = command.data.get("action") {
                            if action == "tts" {
                                if let Some(text) = command.data.get("text").and_then(|v| v.as_str()) {
                                    println!("ðŸŽ¯ [PROXY] Received response: {}", text);
                                    return Ok(text.to_string());
                                }
                            }
                        }
                    }
                }
                Err(e) => {
                    println!("âš ï¸  [PROXY] Poll attempt {} failed: {}", attempt, e);
                }
            }
        }
        
        Err("Timeout waiting for server response".into())
    }

    pub async fn run_proxy_mode(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        // Register with server
        if let Err(e) = self.register().await {
            eprintln!("âŒ [PROXY] Failed to register: {}", e);
            return Err(e);
        }

        println!("ðŸ”„ [PROXY] Starting main loop...");
        
        loop {
            // Poll for commands every 5 seconds
            match self.poll_commands().await {
                Ok(commands) => {
                    for command in commands {
                        println!("ðŸ“‹ [PROXY] Processing command: {:?}", command.data);
                        // Process commands here if needed
                    }
                }
                Err(e) => {
                    eprintln!("âŒ [PROXY] Failed to poll commands: {}", e);
                }
            }

            // Send a heartbeat event
            let heartbeat = Event::new("heartbeat", json!({
                "timestamp": chrono::Utc::now().timestamp(),
                "clientId": self.client_id
            }));

            if let Err(e) = self.send_event(&heartbeat).await {
                eprintln!("âŒ [PROXY] Failed to send heartbeat: {}", e);
            }

            sleep(Duration::from_secs(5)).await;
        }
    }
}

pub struct MultiModeClient {
    llm_service: LLMService,
    config: Config,
}

impl MultiModeClient {
    pub fn new(config_path: &str) -> Result<Self, Box<dyn std::error::Error + Send + Sync>> {
        let config_content = fs::read_to_string(config_path)?;
        let config: Config = serde_json::from_str(&config_content)?;

        let llm_service = match config.mode.as_str() {
            "direct" => {
                LLMService::Direct(DirectLLMService::new(
                    config.openai.clone(), 
                    config.prompt.system.clone()
                ))
            }
            "proxy" => {
                let server_config = config.server_proxy.as_ref()
                    .ok_or("Server proxy config missing for proxy mode")?;
                LLMService::Server(ServerProxyService::new(server_config.clone()))
            }
            _ => return Err(format!("Unknown mode: {}. Valid options: 'direct', 'proxy'", config.mode).into()),
        };

        Ok(Self {
            llm_service,
            config,
        })
    }

    pub async fn process_instruction(&self, text: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
        // Extract instruction from text (simplified)
        let instruction = if text.starts_with("è¯·") || text.starts_with("ä½ ") {
            text
        } else {
            return Ok("Not a valid instruction".to_string());
        };

        self.llm_service.call_llm(instruction).await
    }

    pub async fn run_test_loop(&self) {
        println!("ðŸš€ Multi-Mode Client starting in {} mode", self.config.mode);

        let test_instruction = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±";
        println!("\nðŸ“ Testing: {}", test_instruction);
        
        match self.process_instruction(test_instruction).await {
            Ok(response) => {
                println!("ðŸ“„ Response: {}", response);
            }
            Err(e) => {
                eprintln!("âŒ Error: {}", e);
            }
        }
    }

    pub async fn run_production_mode(&self) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        match &self.llm_service {
            LLMService::Server(proxy_service) => {
                println!("ðŸš€ Starting proxy mode production client...");
                proxy_service.run_proxy_mode().await?;
            }
            LLMService::Direct(_) => {
                println!("ðŸš€ Starting direct mode production client...");
                // For direct mode, we could implement audio processing loop here
                // For now, just run the test
                self.run_test_loop().await;
            }
        }
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let args: Vec<String> = std::env::args().collect();
    
    // Show usage if no config file provided
    if args.len() < 2 {
        print_usage();
        return Ok(());
    }
    
    let config_path = &args[1];
    let test_mode = args.get(2).map(|s| s == "--test").unwrap_or(false);

    println!("ðŸ“‹ Loading config from: {}", config_path);

    let client = MultiModeClient::new(config_path)?;
    
    if test_mode {
        println!("ðŸ§ª Running in test mode");
        client.run_test_loop().await;
    } else {
        println!("ðŸ­ Running in production mode");
        client.run_production_mode().await?;
    }

    Ok(())
}

fn print_usage() {
    println!("ðŸ¤– Open-XiaoAi Unified Client");
    println!();
    println!("Usage:");
    println!("  ./client <config.json> [--test]");
    println!();
    println!("Examples:");
    println!("  ./client config.json         # Run in production mode");
    println!("  ./client config.json --test  # Run in test mode");
    println!();
    println!("ðŸ“– Configuration Setup:");
    println!();
    println!("ðŸ”— Proxy Mode (via HTTP server):");
    println!("{{");
    println!("  \"mode\": \"proxy\",");
    println!("  \"serverProxy\": {{");
    println!("    \"baseURL\": \"http://your-server:4399\",");
    println!("    \"timeout\": 30");
    println!("  }},");
    println!("  \"prompt\": {{");
    println!("    \"system\": \"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚\"");
    println!("  }}");
    println!("}}");
    println!();
    println!("ðŸ¤– Direct Mode (direct LLM API):");
    println!("{{");
    println!("  \"mode\": \"direct\",");
    println!("  \"openai\": {{");
    println!("    \"baseURL\": \"https://api.openai.com/v1\",");
    println!("    \"apiKey\": \"your-api-key\",");
    println!("    \"model\": \"gpt-4\",");
    println!("    \"timeout\": 30,");
    println!("    \"maxTokens\": 1000,");
    println!("    \"temperature\": 0.7");
    println!("  }},");
    println!("  \"prompt\": {{");
    println!("    \"system\": \"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚\"");
    println!("  }}");
    println!("}}");
    println!();
    println!("ðŸ’¡ Tips:");
    println!("  â€¢ Use proxy mode for centralized server management");
    println!("  â€¢ Use direct mode for standalone operation");
    println!("  â€¢ Copy config.template.json and modify for your setup");
    println!("  â€¢ Test your config with --test flag first");
}
